# Core Components of Prometheus

Prometheus is not a single binary, but a monitoring ecosystem composed of several core components that work together to collect, store, query, and act on metrics.

## Correct Answer

**Prometheus, Alertmanager, Pushgateway, Client Libraries, Exporters**

These are the **core components** of the Prometheus ecosystem.

---

## Explanation of Each Core Component

### Prometheus Server

The Prometheus server is the central component of the system.

It is responsible for:

- Scraping metrics from configured targets
- Storing metrics locally in its time-series database (TSDB)
- Evaluating recording rules and alerting rules
- Providing a query interface via PromQL

Without the Prometheus server, no metrics are collected or queried.

---

### Alertmanager

Alertmanager handles alerts generated by Prometheus.

Its responsibilities include:

- Deduplicating alerts
- Grouping related alerts
- Routing alerts to receivers (email, Slack, PagerDuty, etc.)
- Silencing alerts during maintenance windows

Alertmanager does **not** generate alerts itself; it only processes alerts sent by Prometheus.

---

### Pushgateway

The Pushgateway allows short-lived or batch jobs to expose metrics to Prometheus.

Use cases include:

- Cron jobs
- Batch processing tasks
- Jobs that do not run long enough to be scraped reliably

Prometheus scrapes metrics from the Pushgateway instead of scraping the job directly.

---

### Client Libraries

Client libraries are used to instrument application code.

They:

- Provide APIs to define counters, gauges, histograms, and summaries
- Expose metrics via an HTTP endpoint (usually `/metrics`)
- Exist for many languages (Go, Java, Python, Ruby, etc.)

Client libraries make it easy to expose application-level metrics in the Prometheus format.

---

### Exporters

Exporters expose metrics from systems that cannot be instrumented directly.

Examples include:

- Node Exporter (host metrics)
- MySQL Exporter
- PostgreSQL Exporter
- Blackbox Exporter

Exporters translate existing system metrics into Prometheus-compatible metrics.

---

## Why the Other Answers Are Incorrect

### “Prometheus, Alertmanager, Grafana”

This is incorrect because:

- Grafana is **not** a Prometheus component
- Grafana is maintained by Grafana Labs
- Grafana supports many data sources besides Prometheus

Grafana is commonly used with Prometheus, but it is not part of the Prometheus core.

---

### “Prometheus, InfluxDB, Alertmanager”

This is incorrect because:

- InfluxDB is a separate monitoring and time-series database
- It is not part of the Prometheus ecosystem

Prometheus and InfluxDB are alternative solutions, not components of the same system.

---

### “Instrumentation and Exporters”

This answer is incomplete because:

- It omits the Prometheus server itself
- It omits Alertmanager
- It does not describe the full monitoring pipeline

While important, instrumentation and exporters alone do not form a complete Prometheus system.

---

## Summary

- Prometheus consists of multiple cooperating components
- The Prometheus server and Alertmanager are essential
- Client libraries and exporters enable metric exposure
- The Pushgateway supports short-lived jobs
- Visualization tools like Grafana are **external**, not core components

# Querying Firing Alerts in Alertmanager

## Correct Answer

**No such query exists**

There is no PromQL query that can return the alerts currently firing in Alertmanager.

---

## Explanation

Prometheus and Alertmanager are two separate components with distinct responsibilities:

- **Prometheus** evaluates alerting rules and exposes alert state as metrics
- **Alertmanager** receives alerts, deduplicates them, groups them, and routes them to receivers

Because of this separation, **PromQL can only query data that exists in Prometheus**, not Alertmanager’s internal alert state.

---

## The `ALERTS` Time Series

Prometheus exposes a synthetic time series called `ALERTS`.

This time series:

- Represents alerts **as evaluated by Prometheus**
- Reflects the result of alerting rules
- Includes labels such as `alertname` and `alertstate` (`pending`, `firing`)

However, this data:

- Does **not** reflect the current state inside Alertmanager
- Does **not** include alerts sent from sources other than Prometheus
- Does **not** show whether Alertmanager has silenced, grouped, or inhibited an alert

---

## Why the Other Answers Are Incorrect

### `ALERTS`

This is incorrect because:

- It only shows alerts evaluated by Prometheus
- It does not represent alerts currently firing in Alertmanager

---

### `ALERTS{alertstate="firing"}`

This is incorrect because:

- It only filters Prometheus-evaluated alerts in the `firing` state
- Alertmanager may receive alerts from other sources as well

---

### `ALERTS{alertstate="firing", location="alertmanager"}`

This is incorrect because:

- There is no native `location` label on the `ALERTS` time series
- Prometheus does not track where alerts are handled

---

## How to Get Firing Alerts from Alertmanager

The **only supported way** to retrieve currently firing alerts in Alertmanager is via the **Alertmanager API**.

Examples:

- `/api/v2/alerts`
- `/api/v2/silences`

These endpoints expose:

- Active alerts
- Their current status
- Grouping, silencing, and inhibition information

---

## Key Takeaways

- PromQL can only query Prometheus data
- Alertmanager state is **not** available via PromQL
- The `ALERTS` metric reflects Prometheus evaluation only
- Alertmanager must be queried via its HTTP API

# Explanation of the Correct Query

We are given a gauge metric called `cert_expiry` whose value is a **Unix timestamp** (epoch time in seconds) indicating when a certificate will expire.

The goal is to compute the **time remaining until expiration in days**.

---

### Step 1: Understand the Components

- `cert_expiry`  
  → A Unix timestamp (seconds since epoch) representing the expiration time.

- `time()`  
  → A PromQL function that returns the **current Unix time in seconds**.

- `86400`  
  → Number of seconds in one day  
  (`60 × 60 × 24 = 86,400`).

---

### Step 2: Compute the Time Until Expiration

To get the remaining time in **seconds**, we subtract the current time from the expiration timestamp:

```promql
cert_expiry - time()
```

This gives the number of seconds until the certificate expires.

---

### Step 3: Convert Seconds to Days

Since the result is in seconds, we divide by 86400 to convert it to days:

```promql
(cert_expiry - time()) / 86400
```

This query correctly computes the number of days remaining until certificate expiration.

### Summary

By subtracting the current Unix time from the certificate expiration timestamp and converting seconds to days, we obtain the correct time-to-expiry value using PromQL.

# Explanation of the Correct Metric Name

You are instrumenting an application called **`agate`** and exposing metrics to **Prometheus**.  
The application contains a subsystem named **`remed`**, which manages automated remediation processes.

The goal is to define a metric that represents the **number of remediation processes currently running**.

---

### Prometheus Metric Naming Conventions

According to Prometheus best practices:

- Metric names should be:
  - **Lowercase**
  - **Snake_case** (use underscores, not hyphens)
  - **Fully qualified**, meaning they include:
    - The **application name**
    - The **subsystem name**
    - A clear description of what is being measured
- Metric names **must not contain hyphens (`-`)**

---

### Evaluating the Options

- ❌ **`processes_running`**  
  → Not fully qualified; it does not include the application or subsystem name.

- ❌ **`remed_processes_running`**  
  → Missing the application name (`agate`), so it is not fully qualified.

- ❌ **`agate-remed-processes-running`**  
  → Invalid because Prometheus metric names **cannot contain hyphens**.

---

### ✅ Correct Metric Name

```text
agate_remed_processes_running
```

# Alertmanager Functions

The question asks which option is **NOT** a function of **Alertmanager**.

Alertmanager is responsible for handling alerts sent by Prometheus and managing how they are processed and delivered.

---

### Core Functions of Alertmanager

Alertmanager provides several key capabilities, including:

- **Grouping alerts**  
  → Combining related alerts into a single notification to reduce noise.

- **Routing alerts**  
  → Sending alerts to different receivers (email, Slack, PagerDuty, etc.) based on labels and routing rules.

- **Silencing alerts**  
  → Temporarily muting alerts that match specific label sets.

All of these are **native features of Alertmanager**.

---

### Why the Correct Answer Is Correct

- ✅ **Generating text message notifications from alerts**

While Alertmanager can **integrate with external notification providers** (such as services that deliver SMS), it **does not itself generate or send text messages**.

Alertmanager’s role is to **manage, route, and format alerts**, delegating the actual delivery of notifications to external systems or integrations.

---

### Summary of Incorrect Options

- ❌ **Grouping alerts** → Core Alertmanager functionality
- ❌ **Routing alerts** → Core Alertmanager functionality
- ❌ **Silencing alerts** → Core Alertmanager functionality

---

### Explanation of the Correct Answer

The question asks which option **best describes how Prometheus aids in observability**.

Prometheus is a **metrics-based monitoring and observability system**, designed to collect, store, and query time series data.

---

### What Prometheus Actually Does

Prometheus improves observability by:

- **Scraping time series metrics** from applications and infrastructure
- Providing visibility into:
  - System and application **health**
  - **Resource utilization** (CPU, memory, disk, etc.)
  - Performance trends and behavior over time
- Enabling alerting based on metric values and conditions

---

### ✅ Correct Description

> **“By scraping time series metrics, Prometheus gives visibility into the status, resource utilization and much more of infrastructure and applications.”**

This accurately reflects Prometheus’ core purpose and design.

---

### Why the Other Options Are Incorrect

- ❌ **Aggregating logs**  
  → Prometheus **does not ingest or aggregate logs**. Log aggregation is handled by tools like Loki or Elasticsearch.

- ❌ **Receiving pushed events**  
  → Prometheus works primarily via **pull-based scraping**, not by receiving pushed events, and it collects metrics, not discrete events.

- ❌ **Tracing network calls**  
  → Prometheus **does not collect distributed traces**. Tracing is handled by tools like Jaeger or Zipkin.

---

### Summary

Prometheus contributes to observability by collecting and analyzing **time series metrics**, offering deep insight into the behavior and performance of systems and applications over time.

---

### Explanation of Binary Operator Precedence in PromQL

PromQL defines a set of **binary operators** that can be used in queries.  
Each operator has a specific **precedence**, which determines the order in which expressions are evaluated if parentheses are not used.

---

### Step 1: Understanding Operator Precedence

The **general precedence rules** in PromQL are:

1. **Exponentiation (`^`)** → highest precedence
2. **Multiplication (`*`), Division (`/`), Modulo (`%`)**
3. **Addition (`+`), Subtraction (`-`)**
4. **Comparison operators (`==`, `!=`, `<`, `>`, `<=`, `>=`)**
5. **Logical AND (`and`)**
6. **Logical OR (`or`)** → lowest precedence

> Operators with higher precedence are evaluated **before** operators with lower precedence.

---

### Step 2: Evaluating the Options

- ✅ **`^, *, -, >, and, or`**

  - Exponentiation → highest
  - Multiplication → next
  - Subtraction → after multiplication
  - Comparison (`>`) → lower than arithmetic
  - Logical AND → lower than comparison
  - Logical OR → lowest  
    ✅ This matches the correct decreasing precedence order.

- ❌ **`%, ^, /, ==`**

  - Incorrect because **`^`** has higher precedence than `%` or `/`.

- ❌ **`*, +, !=, or, and`**

  - Incorrect because **`and`** has **higher precedence than `or`**, but here it appears after `or`.

- ❌ **`or, and, !=, -, *`**
  - Incorrect as the order is completely reversed; the lowest precedence operator (`or`) is listed first.

---

### Explanation of When Distributed Tracing Is Not Particularly Useful

Distributed tracing is designed to track **requests as they travel across multiple services or components**. It provides visibility into latency, bottlenecks, and failures in **multi-service architectures**.

---

### Step 1: Understanding Distributed Tracing Use Cases

Distributed tracing is most beneficial when:

- Requests flow through **multiple services or systems**
- You need to **identify performance bottlenecks** or **errors** across service boundaries
- You want **end-to-end visibility** of a transaction

---

### Step 2: Evaluating the Scenarios

1. **A 3-layer web architecture (frontend, backend, database)**

   - ✅ Multi-service → Distributed tracing is useful

2. **A monolithic web application**

   - ✅ Single service → Distributed tracing is **less useful**
   - While internal spans can be used to trace sections of the code, the main benefit of distributed tracing is lost because requests do not cross service boundaries.

3. **A batch job that uses an API and a database**

   - ✅ Multiple systems → Distributed tracing is useful for identifying delays or errors between the API and database

4. **A customer interaction spanning an internal data center and a public cloud**
   - ✅ Multi-service, multi-location → Distributed tracing is very useful to track latency or failures across environments

---

### Explanation of Alertmanager’s Policy for Suppressing Related Alerts

The question asks for the **Alertmanager term** describing a policy where **notifications for one type of alert are suppressed if another alert is firing**.

---

### Step 1: Understand Alertmanager Features

Alertmanager provides several mechanisms for managing alerts:

1. **Grouping**

   - Combines related alerts into a single notification to reduce noise.

2. **Silencing**

   - Temporarily mutes alerts based on label match.
   - **Independent of other alerts**; a silence is manually defined.

3. **Routing**

   - Determines where alerts are sent based on their labels.

4. **Inhibition**
   - Suppresses notifications for one alert **when another alert is active**.
   - Example: Suppress “DiskFull” alerts if a higher-priority “NodeDown” alert is firing.

---

### Step 2: Evaluate the Options

- ❌ **Grouping** → Only combines alerts, does not suppress based on other alerts.

- ❌ **Silencing** → Mutes alerts manually, not conditionally based on other alerts.

- ✅ **Inhibiting** → Correct. Suppresses notifications **conditionally**, based on the firing of another alert.

- ❌ **Routing** → Determines receivers, does not suppress alerts.

---

### Explanation of Valid Prometheus Label Values

Prometheus **labels** consist of a **name** and a **value**.  
Label values have very **flexible rules**:

- Can contain **any Unicode characters**
- Can include letters, numbers, underscores, hyphens, and even special characters (e.g., `$`)
- Can start with **any character**, including numbers or special characters

> Unlike **label names**, which must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`, label **values have no such restrictions**.

---

### Step 1: Evaluate the Options

1. **`val`**

   - ✅ Valid: simple alphabetic string

2. **`label-value-1`**

   - ✅ Valid: contains hyphens and numbers, which are allowed

3. **`2nd_label_value`**

   - ✅ Valid: contains numbers and underscores, and can start with a number

4. **`$__maybe_valid`**
   - ✅ Valid: contains special characters like `$` and `_`; allowed since label values can include any Unicode character

---

### Step 2: Key Points

- **Label values** are much more permissive than label names
- They can include **special characters, numbers at the start, hyphens, and even emojis**

---

### Explanation of the Correct Service Discovery Method for Node Exporter on AWS EKS

You have deployed **Node Exporter** on EC2 instances in an **Elastic Kubernetes Service (EKS)** cluster, but Node Exporter is managed by **systemd** on the host, not as Kubernetes pods.

---

### Step 1: Understanding Service Discovery Options in Prometheus

Prometheus supports several service discovery methods:

1. **`kubernetes_sd_configs`**

   - Discovers **Kubernetes objects** (pods, services, nodes) via the Kubernetes API
   - ❌ Not useful here because Node Exporter is **not deployed as a pod**. No Node Exporter pods exist to discover.

2. **`file_sd_configs`**

   - Reads target definitions from a **static file**
   - ❌ Less dynamic; requires manually updating the file when EC2 instances change

3. **`static_sd_configs`**

   - Defines targets **statically in the Prometheus configuration**
   - ❌ Not ideal for dynamic cloud environments like EKS, where nodes can be added or removed frequently

4. **`ec2_sd_configs`**
   - Uses the **AWS EC2 API** to dynamically discover EC2 instances
   - ✅ Ideal for Node Exporter deployed on EC2 hosts
   - Automatically updates targets when instances are added or removed

---

### Step 2: Why `ec2_sd_configs` is the Best Choice

- Node Exporter runs **on EC2 instances**, not in Kubernetes pods
- Nodes in EKS can **scale dynamically**, so static configs would be hard to maintain
- `ec2_sd_configs` leverages **built-in Prometheus support for AWS**, automatically discovering instances with proper tags or metadata

---

### Explanation of Histograms vs Summaries in Prometheus

Prometheus supports two types of **metrics for observing distributions**: **histograms** and **summaries**. Understanding their differences is important for aggregation and quantile calculations.

---

### Step 1: How Histograms Work

- Histograms collect **bucketed observations**, counting how many observations fall into predefined ranges.
- **Buckets** and a **count/total sum** are exposed to Prometheus.
- **Aggregation:** Since histograms expose bucketed counts, they **can be aggregated across multiple instances** using PromQL functions like `sum()` or `rate()`.
- Quantiles can be calculated on the **server side** by PromQL using `histogram_quantile()`.

---

### Step 2: How Summaries Work

- Summaries track **quantiles and total counts/sums** on the client side.
- Quantile calculations are **performed at the application level** (client-side) before being exposed to Prometheus.
- **Aggregation limitation:** Summaries generally **cannot be aggregated across multiple instances** for accurate quantiles because each summary only exposes its local observations and calculated quantiles, not the raw distribution.

---

### Step 3: Evaluating the Options

- ❌ **“Histogram metrics generally cannot be aggregated”**  
  → Incorrect; histograms **can be aggregated** using their buckets.

- ✅ **“Summary metrics generally cannot be aggregated”**  
  → Correct; client-side quantiles prevent accurate aggregation across multiple instances.

- ❌ **“Histogram metrics are evaluated on the client side”**  
  → Incorrect; histograms expose buckets to Prometheus, and quantiles are computed **server-side**.

- ❌ **“Summary metrics are evaluated on the server side”**  
  → Incorrect; summary quantiles are computed **client-side**.

---

### Explanation of the Relationship Between SLA, SLO, and SLI

In service reliability terminology, **SLA**, **SLO**, and **SLI** are related but distinct concepts:

---

### Step 1: Define Each Term

1. **SLI (Service Level Indicator)**

   - A **quantitative measure** of some aspect of the service’s performance
   - Examples: request latency, error rate, uptime percentage

2. **SLO (Service Level Objective)**

   - A **target value or goal** for an SLI
   - Example: “99.9% of requests should have latency under 200ms”

3. **SLA (Service Level Agreement)**
   - A **formal agreement** between a service provider and a customer
   - Often references **SLOs** and specifies **consequences** if the SLOs are not met

---

### Step 2: Understand the Hierarchy

- **SLA > SLO > SLI**
  - **SLIs** are the **building blocks** (metrics we measure)
  - **SLOs** are **goals** based on SLIs
  - **SLAs** are **agreements** that reference SLOs and define obligations or penalties

---

### Step 3: Evaluate the Options

- ❌ **SLI > SLO > SLA**  
  → Incorrect; the hierarchy is reversed

- ❌ **SLO > SLA > SLI**  
  → Incorrect; SLAs are broader than SLOs, not narrower

- ✅ **SLA > SLO > SLI**  
  → Correct; reflects the proper hierarchy and relationship

- ❌ **SLA > SLI > SLO**  
  → Incorrect; SLOs are defined **using SLIs**, not the other way around

---

### Explanation of the Ideal Exporter for Monitoring Network Devices

Prometheus uses **exporters** to collect metrics from systems that do not natively expose Prometheus metrics. Different exporters are optimized for different types of systems.

---

### Step 1: Evaluate the Exporters

1. **Node Exporter**

   - Designed to monitor **Linux servers**
   - Provides metrics like CPU, memory, disk, and network statistics of the host
   - ❌ Not ideal for dedicated network devices like routers or switches

2. **SNMP Exporter**

   - Uses **SNMP (Simple Network Management Protocol)** to scrape metrics from **network devices**
   - Ideal for routers, switches, firewalls, and other network hardware
   - ✅ Correct choice

3. **JMX Exporter**

   - Designed to expose metrics from **Java Virtual Machines (JVMs)**
   - ❌ Not relevant for network devices

4. **SMTP Exporter**
   - SMTP relates to **email protocol**, not network monitoring
   - ❌ Confusingly similar acronym to SNMP, but not used for network devices

---

### Step 2: Why SNMP Exporter is the Best Choice

- Network devices often support **SNMP** as the standard protocol for monitoring
- The **SNMP Exporter** converts SNMP metrics into **Prometheus-compatible metrics**
- Enables monitoring of interface statistics, device uptime, errors, and more

---

### Explanation of the Definition of a Metric

In observability, it is important to distinguish between **metrics, time series, and logs**.

---

### Step 1: What is a Metric?

- A **metric** is a **numeric measurement** that represents some aspect of a system or application.
- Metrics are usually **quantitative values** like CPU usage, request latency, or memory usage.

---

### Step 2: Clarifying Common Confusions

- ❌ **“A numeric measurement made over time”**  
  → This is a **time series**, which is a metric recorded at multiple points in time.

- ❌ **“Structured textual data regarding an event”**  
  → This describes a **log**, not a metric.

- ❌ **“A description of an anomalous state”**  
  → Metrics may indicate anomalies, but a metric itself is **just a measurement**, not inherently anomalous.
