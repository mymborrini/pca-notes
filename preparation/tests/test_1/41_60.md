### Explanation: Purpose of Prometheus Recording Rules

Prometheus **recording rules** are used to precompute and store the results of PromQL expressions as new metrics, improving query efficiency and enabling reuse across dashboards and alerts.

---

### Step 1: Define Recording Rules

- Recording rules are **PromQL expressions** that are evaluated at regular intervals.
- The results are **saved back to the Prometheus time series database (TSDB)** as a new metric.
- This allows:
  - Faster queries for complex or frequently used expressions
  - Reuse of computed metrics in other queries or alerting rules

---

### Step 2: Evaluate the Options

- ❌ **“Record the current state of all firing alerts for later analysis”**

  - Incorrect; recording rules do **not handle alerts**. Alerts are handled by **alerting rules**.

- ✅ **“Evaluate a PromQL expression and save the result back to the TSDB as a new metric”**

  - Correct; this is the exact purpose of recording rules.

- ❌ **“Record the screen of users of an application and generate metrics from their session”**

  - Incorrect; this describes **Real User Monitoring**, not Prometheus recording rules.

- ❌ **“Define criteria for recording details of metrics for later retroactive evaluation of alerting rules”**
  - Incorrect; alerting rules are **evaluated in real time**, not retroactively.

---

### Explanation: Purpose of Prometheus Recording Rules

Prometheus **recording rules** are used to precompute and store the results of PromQL expressions as new metrics, improving query efficiency and enabling reuse across dashboards and alerts.

---

### Step 1: Define Recording Rules

- Recording rules are **PromQL expressions** that are evaluated at regular intervals.
- The results are **saved back to the Prometheus time series database (TSDB)** as a new metric.
- This allows:
  - Faster queries for complex or frequently used expressions
  - Reuse of computed metrics in other queries or alerting rules

---

### Step 2: Evaluate the Options

- ❌ **“Record the current state of all firing alerts for later analysis”**

  - Incorrect; recording rules do **not handle alerts**. Alerts are handled by **alerting rules**.

- ✅ **“Evaluate a PromQL expression and save the result back to the TSDB as a new metric”**

  - Correct; this is the exact purpose of recording rules.

- ❌ **“Record the screen of users of an application and generate metrics from their session”**

  - Incorrect; this describes **Real User Monitoring**, not Prometheus recording rules.

- ❌ **“Define criteria for recording details of metrics for later retroactive evaluation of alerting rules”**
  - Incorrect; alerting rules are **evaluated in real time**, not retroactively.

---

### Explanation: Metrics Automatically Generated by Prometheus During a Scrape

When Prometheus scrapes a target, it automatically generates several **internal metrics** to monitor the scraping process itself. These metrics help operators understand Prometheus performance and detect issues with scrapes.

---

### Step 1: Key Automatically Generated Metrics

1. **`up`**

   - Indicates whether a scrape was successful (1) or failed (0)
   - ✅ Automatically generated

2. **`scrape_duration_seconds`**

   - Measures how long a scrape took in seconds
   - ✅ Automatically generated

3. **`scrape_samples_scraped`**

   - Counts the number of samples scraped from a target
   - ✅ Automatically generated

4. **`scrape_payload_bytes`**
   - ❌ Not an automatically generated metric
   - There is **no Prometheus metric that directly reports payload size**; if needed, it would have to be instrumented manually in the target or exporter.

---

### Step 2: Evaluate the Options

- ❌ **`up`** → Automatically generated
- ❌ **`scrape_duration_seconds`** → Automatically generated
- ✅ **`scrape_payload_bytes`** → Not automatically generated
- ❌ **`scrape_samples_scraped`** → Automatically generated

---

### Explanation: Querying the Duration of the Last Scrape for a Specific Job

Prometheus exposes internal metrics to monitor its own scraping process. One key metric is **`scrape_duration_seconds`**, which represents the duration of the last scrape of a target in seconds.

---

### Step 1: Understanding the Metric

- **`scrape_duration_seconds`**

  - Measures the **duration of the last scrape** for each target
  - Automatically generated by Prometheus for **every target**

- **Filtering by job**
  - Use **label selectors** to filter the metric for a specific job
  - Syntax: `{label="value"}`

---

### Step 2: Evaluate the Options

1. **`scrape_duration_seconds`**

   - ❌ Returns all scrape durations for all jobs, not limited to the `node` job

2. **`scrape_duration_seconds{job="node"}`**

   - ✅ Correct
   - Returns the duration in seconds of the last scrape for **each instance of the `node` job** only

3. **`last_scrape_duration{job="node"}`**

   - ❌ Metric does not exist in Prometheus by default

4. **`last_scrape_duration_seconds{job="node"}`**
   - ❌ Metric does not exist in Prometheus by default

---

### Step 3: Key Point

- **Label selectors** are used to filter metrics to a specific subset of targets.
- **`scrape_duration_seconds{job="node"}`** gives exactly the last scrape duration for all instances of the Node Exporter job.

---

### Explanation: Required Query Parameters for a Blackbox Exporter Probe

The **Prometheus Blackbox Exporter** allows probing endpoints (HTTP, TCP, ICMP, etc.) and exposing the results as metrics. To perform a probe, certain **query parameters** must be specified in the probe request.

---

### Step 1: Key Query Parameters

1. **`target`**

   - The **URL or IP address** of the endpoint to probe
   - Example: `target=https://example.com`

2. **`module`**
   - Specifies the **probe configuration** to use from the Blackbox Exporter configuration file
   - Defines **probe type** (http, tcp, icmp) and other settings
   - Example: `module=http_2xx`

---

### Step 2: Evaluate the Options

- ❌ **“instance and profile”** → Not valid parameters for Blackbox Exporter
- ❌ **“target and profile”** → Should be `module` instead of `profile`
- ❌ **“instance and module”** → `instance` is not used; `target` is required instead
- ✅ **“target and module”** → Correct; these are the two required query parameters

---

### Step 3: Example Blackbox Exporter Probe URL

```text
http://blackbox-exporter:9115/probe?target=https://example.com&module=http_2xx
```

- target = the endpoint to probe
- module = the probe configuration

---

### Explanation: Choosing the Right Metric for In-Flight HTTP Requests

When monitoring an HTTP API, you may want to track the **number of in-flight requests**—requests that have been received but not yet completed.

---

### Step 1: Metric Types

1. **Counter**

   - Only increases over time (monotonic)
   - Good for tracking totals, e.g., `http_requests_total`
   - ❌ Not suitable for in-flight requests because they **can go up and down**.

2. **Gauge**
   - Can **increase or decrease arbitrarily**
   - Suitable for tracking **current values**, such as in-flight requests, memory usage, or queue length
   - ✅ Correct choice

---

### Step 2: Metric Naming

- Metric names should accurately describe what is being measured:
  - `http_requests_in_flight` → describes the **current number of ongoing requests**
  - `http_requests_total` → describes **cumulative requests**, not in-flight requests

---

### Step 3: Evaluate the Options

- ❌ **`http_requests_in_flight, Counter`** → Counter is inappropriate
- ❌ **`http_requests_total, Counter`** → Tracks cumulative requests, not in-flight
- ✅ **`http_requests_in_flight, Gauge`** → Correct: reflects current number of in-flight requests and allows increase/decrease
- ❌ **`http_requests_total, Gauge`** → Name does not accurately describe in-flight requests

---

### Explanation: How and at What Level Prometheus Data Retention Is Configured

Prometheus stores time series data in its **TSDB (Time Series Database)**. The **retention policy** determines how long this data is kept.

---

### Step 1: Configuration Method

- Retention is configured **globally** using **command-line flags** when starting the Prometheus server.
- Key flag: `--storage.tsdb.retention.time`
  - Example: `--storage.tsdb.retention.time=15d`
  - This keeps **15 days of data** and automatically deletes older data.

---

### Step 2: Granularity

- Retention is **global**, not per scrape or per target.
- Prometheus does **not support per-job or per-metric retention**. All data is managed together in the TSDB.

---

### Step 3: Evaluate the Options

- ❌ **“Globally via the configuration file”** → Incorrect; retention is not set in `prometheus.yml`
- ✅ **“Globally via command line flags”** → Correct; retention is configured at the global TSDB level
- ❌ **“Per-scrape in the configuration file”** → Incorrect; retention is not per scrape
- ❌ **“Per-scrape in command line flags”** → Incorrect; retention cannot be set per scrape

---

### Explanation: Triggering a Prometheus Configuration Reload

Prometheus can reload its configuration **without restarting the server**, allowing changes to take effect dynamically.

---

### Step 1: Methods to Reload Configuration

1. **Send a SIGHUP signal to the Prometheus process**
   - ✅ Correct method
   - Example (Linux/macOS):
   ```bash
   kill -HUP <prometheus_pid>
   ```

- Prometheus will reload prometheus.yml and any rule files.

2. **HTTP Reload Endpoint**

- Available only if the server is started with the --web.enable-lifecycle flag
- Must use an HTTP POST or PUT request, not GET
- Example:

```bash
curl -X POST http://localhost:9090/-/reload
```

### Step 2: Why Other Options Are Incorrect

- ❌ POST to /-/reload with --web.enable-admin-api
  - Incorrect flag; must use --web.enable-lifecycle
- ❌ GET to /-/reload with --web.enable-admin-api
  - GET is not allowed; must use POST or PUT
- ❌ GET to /-/reload with --web.enable-enable-lifecycle
  - Typo in flag and wrong HTTP method; must be --web.enable-lifecycle and POST/PUT

### Explanation: Disabling Alert Grouping in Alertmanager

Alertmanager groups alerts into notifications based on the **`group_by`** configuration in a route. To effectively **disable grouping**, you need to configure `group_by` to consider each alert individually.

---

### Step 1: How `group_by` Works

- `group_by` specifies the list of labels used to **group alerts** into a single notification.
- Alerts with identical values for all `group_by` labels are **combined** into one notification.
- By default, grouping uses `['alertname']`.

---

### Step 2: Disabling Grouping

- To **disable grouping**, set `group_by` to `['...']`.
  - The special label `'...'` acts as a wildcard that **matches all labels**, making each alert appear in its own group.
- Example:

```yaml
route:
  receiver: "team-X"
  group_by: ["..."] # effectively disables grouping
```

- This ensures that every alert is sent as an individual notification, regardless of labels.

### Step 3: Evaluate Other Options

- ❌ disable_grouping: true → Not a valid Alertmanager configuration option
- ❌ enable_grouping: false → Not a valid Alertmanager configuration option
- ❌ group_by: ['*'] → Incorrect syntax; Alertmanager uses '...' instead of '\*'
