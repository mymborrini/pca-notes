# Instrumenting Code

Adding prometheus metrics directly to a codebase means instrumenting the code. This enabled what is generally refers as whitebox monitoring where system reports metrics about his own behaviour instead of been monitoring by outside

## Client libraries

Promehteus offers vlient libraries for multiple languages. These libraries fulfill two functions:

- track metrics about what is happening in your application
- expose the tracked metrics to prometheus over HTTP

## Metric Types

Prometheus client libraries allow you to track the state of your service with a number of different metric types. 

- Counters
- Gauges
- Histogram
- Summaries

Summaries calculate client-side-calculated quntiles form a set of observations, like request latency percentiles. They also track the total count and the total sum of observations (like the total count of requests and the total sum of requests durations).

Histogram and Summary metric can both be used for calculating quantiles but have different trade-offs. The most important one is that Summary metrics cannot be aggregated over dimensions or multiple instances. 




----

## Histogram vs Summary

### Histogram

A histogram in Prometheus is a metric type that counts observations distributed into ‚Äúbuckets.‚Äù

#### How it works:

When you record a value (for example, an HTTP request latency), Prometheus places it into one or more predefined buckets.
Each bucket has an upper bound and keeps a count of how many observations are less than or equal to that bound.

#### Example of buckets:

le=0.1
le=0.5
le=1
le=5
+Inf


So we have 5 buckets for this metric Each observation is compared against these limits and increments the count for the corresponding buckets.

#### Metrics generated by a histogram:

For a histogram called http_request_duration_seconds, you will get:

http_request_duration_seconds_bucket{le="0.1"} 123
http_request_duration_seconds_bucket{le="0.5"} 456
http_request_duration_seconds_bucket{le="1"} 789
http_request_duration_seconds_bucket{le="5"} 1000
http_request_duration_seconds_sum 320
http_request_duration_seconds_count 100

sum ‚Üí the total of all observed values
count ‚Üí the total number of observations

#### Pros

You can calculate percentiles and latency distributions using PromQL (histogram_quantile(0.95, rate(...[5m])))
Great for aggregating data across multiple instances

#### Cons

Percentiles are approximated, depending on the bucket boundaries

### Summary

A summary is a metric that directly calculates percentiles (quantiles) and sums.

#### How it works?

When you record a value, the summary keeps track internally of the sum, count, and configured quantiles (e.g. p50, p90, p99).

#### Example

http_request_duration_seconds{quantile="0.5"} 0.23
http_request_duration_seconds{quantile="0.9"} 0.85
http_request_duration_seconds{quantile="0.99"} 1.2
http_request_duration_seconds_sum 320
http_request_duration_seconds_count 100

quantile ‚Üí directly computed percentile values (not based on buckets)

#### Pros

Provides accurate percentiles for a single instance
Useful when you need precise p50/p95/p99 values

#### Cons:

Cannot be aggregated across instances (percentiles don‚Äôt combine correctly)
Best used for per-instance metrics

### üìä Key differences

| Feature           | Histogram                                     | Summary                             |
| ----------------- | --------------------------------------------- | ----------------------------------- |
| Percentiles       | Calculated via buckets (approximate)          | Calculated directly (more accurate) |
| Aggregation       | Easy across multiple instances                | Hard to aggregate across instances  |
| Metrics generated | bucket, sum, count                            | quantile, sum, count                |
| Typical use case  | Multi-pod aggregations, Prometheus dashboards | Per-application percentiles         |

### üí° Rule of thumb:

Need to aggregate metrics from multiple instances ‚Üí use histogram
Need precise percentiles from a single instance ‚Üí use summary

## Best Practice

When adding metrics to your own code, there are a number of recommended best practices to follow.
These are documented in Prometheus's best practices documentation for metric and label naming, as well as for instrumentation.

- This is a summary of the most important points:
- Include a unit suffix in the metric name.
- Use base units (e.g. use seconds vs. milliseconds, bytes vs. megabytes).
- Add a _total suffix to counters, no suffix for gauges.
- Avoid labels with an unbounded number of values (like user IP addresses). Every label value creates a new time series that needs to be tracked.
- If the dimensions of a multi-dimensional metric are easy to enumerate at startup, initialize each of them explicitly (for example, initialize all counter dimensions to 0). This avoids missing series, which can be troublesome to work with.
- Expose counters for errors and total requests rather than counters for errors and successful requests. It‚Äôs easier for ratio calculations like failed / total than failed / (succeeded + failed).
- As a rule of thumb, either the sum() or the avg() over all series of a metric name should be meaningful (the dimensions should partition the metrics space). Otherwise, split dimensions into separate metric names.

Try to keep these best practices in mind when instrumenting the example code in the next section.

